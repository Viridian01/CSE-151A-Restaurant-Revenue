# CSE-151A-Restaurant-Revenue    üçΩ‚òïÔ∏èüçª

## Milestone 2
Gateway to Milestone 2:
<a target="_blank" href="https://github.com/Viridian01/CSE-151A-Restaurant-Revenue/tree/milestone2">
  <img src="https://img.shields.io/badge/Milestone%202-Click%20here?logo=github&labelColor=grey" alt="Open In Github"/>
</a>

## Milestone 3
Gateway to Milestone 3:
<a target="_blank" href="https://github.com/Viridian01/CSE-151A-Restaurant-Revenue/tree/milestone3">
  <img src="https://img.shields.io/badge/Milestone%203-Click%20here?logo=github&labelColor=grey" alt="Open In Github"/>
</a>


## Jupyter Notebook
Gateway to the Jupyter Notebook:
<a target="_blank" href="https://colab.research.google.com/github/Viridian01/CSE-151A-Restaurant-Revenue/blob/milestone4/main.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

## Written Report
Here is PDF version of our written report:
<a target="_blank" href="">
   <img src="https://img.shields.io/badge/Download%20as%20PDF-EF3939?style=flat&logo=adobeacrobatreader&logoColor=white&color=black&labelColor=ec1c24">
</a>

## Environment setup instructions:
1. Link to download data from [Kaggle](https://www.kaggle.com/datasets/anthonytherrien/restaurant-revenue-prediction-dataset) or [here](data/restaurant_data.csv)
2. Unzip the file and upload it into the Google Colab
3. Imports needed for data processing

   ```
   import numpy as np
   import pandas as pd
   import matplotlib.pyplot as plt
   import seaborn as sns
   ```

## Introduction
<p style="margin-top: 20px;">&emsp; Do you ever wonder why some restaurants are more successful than others? Could it be the type of cuisine, consumer preferences, or menu prices? We hope to answer this question by developing a model that aims to predict our measure of success, and total revenue, utilizing many attributes about a restaurant. We chose this area of focus for a variety of reasons. First, the idea of predicting the success of an institution in the food sector immediately appeals to us as both a topic not heavily explored, as well as one that can personally affect us as consumers. Many of the applications of predictive machine learning models are in large companies and institutions, but restaurants can be locally owned, even by the people close to us. Furthermore, our dataset is clean and contains a large enough sample size with enough features, which are both vital aspects for training a performant model. Through this project, we hope not only to better understand the financial aspects of food but also to create a useful tool for small business owners to fine-tune their restaurants to maximize their revenue.</p>

## Methods
### Data Exploration
We first introduce our dataset, with examples shown in [Table 1](#preprocessing). The full dataset can be found at this [URL](https://colab.research.google.com/github/Viridian01/CSE-151A-Restaurant-Revenue/blob/milestone4/main.ipynb). Our dataset has 8368 observations and 17 features (including the target, and revenue). The features are as follows:

   - `Name` ‚Äî serves as an identifier for each restaurant.
   - `Location` ‚Äî gives the location category of the restaurant, either rural, downtown, or suburban.
   - `Cuisine` ‚Äî gives the type of cuisine the restaurant serves, either Japanese, Mexican, Italian, Indian, French, or American.
   - `Rating` ‚Äî gives the average rating of the restaurant on a five-star scale.
   - `Seating Capacity` ‚Äî gives the total number of seats in the restaurant.
   - `Average Meal Price` ‚Äî gives the average price of a meal at the restaurant (in an unknown currency--we assume USD).
   - `Marketing Budget` ‚Äî gives the budget allocated towards marketing by the restaurant (in an unknown unit--we assume USD per month).
   - `Social Media Followers` ‚Äî gives the total number of followers on the restaurant's social media.
   - `Chef Experience Years` ‚Äî gives the number of years of experience of the head chef.
   - `Number of Reviews` ‚Äî gives the total number of reviews the restaurant has received.
   - `Avg Review Length` ‚Äî gives the average length of a review (in an unknown unit--we assume words).
   - `Ambience Score` ‚Äî gives the score of the restaurant's ambience on a 10-point scale.
   - `Service Quality Score` ‚Äî gives the score of the restaurant's service on a 10-point scale.
   - `Parking Availability` ‚Äî denotes whether parking is available at the restaurant or not.
   - `Weekend Reservations` ‚Äî gives the number of total weekend reservations in an average week.
   - `Weekday Reservations` ‚Äî gives the number of total weekday reservations in an average week.
   - `Revenue` ‚Äî gives the total revenue generated by the restaurant (in an unknown unit--we assume USD per year).

Note that we make some assumptions about our dataset's features, the motivation for which is detailed in our notebook. Our notebook also details some further data exploration steps: however, crucial findings are later discussed.

<div align="center">
	<img width = "50%" src="assets/heatmap.png">
  <p><em>Heatmap</em></p>
</div>

<div align="center">
	<img width = "80%" src="assets/pairplot.png">
  <p><em>Pairplot</em></p>
</div>

### Preprocessing
Our dataset is already quite clean, so little preprocessing is required. Notably, our dataset contains no missing data, so imputation is not required. Our only preprocessing steps are the following:
   1. We encode the categorical features. For `Cuisine` and `Location`, we used One-Hot encoding. For `Parking Availability`, we simply encode it as 0 and 1, as the feature is a boolean value.

      | Name                      | Restaurant 0 | Restaurant 1 | Restaurant 2 |
      |---------------------------|--------------|--------------|--------------|
      | Location                  | Rural        | Downtown     | Rural        |
      | Cuisine                   | Japanese     | Mexican      | Italian      |
      | Rating                    | 4.0          | 3.2          | 4.7          |
      | Seating Capacity          | 38           | 76           | 48           |
      | Average Meal Price        | 73.98        | 28.11        | 48.29        |
      | Marketing Budget          | 2224         | 4416         | 2796         |
      | Social Media Followers    | 23406        | 42741        | 37285        |
      | Chef Experience Years     | 13           | 8            | 18           |
      | Number of Reviews         | 185          | 533          | 853          |
      | Avg Review Length         | 161.92       | 148.76       | 56.85        |
      | Ambience Score            | 1.3          | 2.6          | 5.3          |
      | Service Quality Score     | 7.0          | 3.4          | 6.7          |
      | Parking Availability      | Yes          | Yes          | No           |
      | Weekend Reservations      | 13           | 48           | 27           |
      | Weekday Reservations      | 4            | 6            | 14           |
      | Revenue                   | 638945       | 490208       | 541369       |
      
      _Table 1: The table shows the first 3 examples in the dataset, along with all of their features._

   2. We scale the data, using min-max normalization. Our data exploration shows that many of the features are not normally distributed, so this kind of scaling is more logical than standardization.

Our data exploration also shows that many features are already very linearly correlated with `Revenue`, so we do not feel the need to expand our feature set any further. Finally, we split our dataset into a training and testing subset.
 
### Model Selection
To select a model, we compare a variety of regression models using cross-validation on our training set. The models chosen are simple linear regression, k-nearest neighbors (KNN), a decision tree, a random forest, a support vector regressor (SVR), and a multilayer perceptron (MLP). After determining which model yields the best evaluation metric, for which we use root mean squared error (RMSE), we perform further hyperparameter tuning to determine whether our model can be improved further. The results of our model selection process are discussed in the next section.

## Results
The cross-validation results of our models are shown in Figure `Model Performance Histogram`. Note that the RMSE here is scaled and not an interpretable value relative to the true revenue values.

<div align="center">
	<img width = "50%" src="assets/models_performance.png">
  <p><em>Model Performance Histogram</em></p>
</div>

We tune hyperparameters only for the random forest model and the MLP. Performing a grid search over two hyperparameters for the random forest yields the results shown in Figure [figure]. 

<!--- gridsearch figure here --->

Additionally, we tune the number of units within each layer (excluding the output layer) for the MLP to further optimize the model.


Finally, after tuning both model candidates, the random forest achieves about \$27651 RMSE on the training set and \$65764 on the testing set, while the MLP achieves \$69303 RMSE on the training set and \$71011 on the testing set. For reference, the mean revenue across the dataset is about \$656000. The fitting results are shown in Figure [figure], which displays both the true and predicted values for both models.

<!--- graph figure here --->

To gain insights into which features contribute the most to our Random Forest model's predictions, we examined the feature importance. The features with the highest importance were the average meal price, seating capacity, and marketing budget. This suggests that these factors play a significant role in determining a restaurant's revenue, aligning with intuitive expectations about the impact of pricing, capacity, and marketing efforts on financial performance.

## Discussion
### Model Selection and Evaluation
Our model selection process was relatively straightforward due to the strong linear correlations between many of the features and the target variable, revenue. This linearity allowed even simpler models to perform reasonably well without extensive tuning. The random forest model stood out, achieving an RMSE of \$27651 on the training set and \$65764 on the testing set. The slight overfitting observed suggests that while the model captures patterns well, it also picks up noise in the training data. The MLP model, with an RMSE of \$69303 on the training set and \$71011 on the testing set, performed slightly worse but validated the robustness of our feature set.

### Data Assumptions and Limitations
We made several assumptions regarding the dataset, such as assuming all currency values were in USD and interpreting various features like marketing budget and reservations on a monthly and weekly basis, respectively. These assumptions could introduce inaccuracies if the actual units differ. The dataset's unusually clean and consistent nature raises the possibility that it might be synthetic, which would limit the generalizability of our models to real-world scenarios.

### Shortcomings
A major shortcoming is the dataset's lack of diversity, containing only higher-end restaurants with average meal prices above \$25 and excluding many common cuisines. This limits our model's applicability to a broader range of restaurants. Additionally, hyperparameter tuning yielded minimal improvements, suggesting that our initial models were already near-optimal or that our search space was insufficiently extensive.

## Conclusion
